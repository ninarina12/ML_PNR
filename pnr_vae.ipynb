{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93537559",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/ninarina12/ML_PNR/blob/master/pnr_vae.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949aa9d",
   "metadata": {},
   "source": [
    "### Only if running in Google Colaboratory:\n",
    "- Go to Runtime > Change runtime type, and select GPU.\n",
    "- Clone the GitHub repository to access supplementary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ninarina12/ML_PNR.git\n",
    "%cd ML_PNR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec2078",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fde977",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#from plot_sld import plot_SLD\n",
    "\n",
    "from utils.utils_data import (u, parse_metadata, parse_labels, get_exp_names, read_exp, process_data, process_exp,\n",
    "                              normalize_log, split_data, drop_duplicates_list, convert_units)\n",
    "from utils.utils_plot import format_axis, fontsize, textsize, cmap, cmap_temp, cmap_cm, cmap_mse, set_colors, palette\n",
    "from utils.utils_model import (TorchStandardScaler, VAE, CVAE, init_model, train, evaluate,\n",
    "                               get_predictions, get_predictions_exp)\n",
    "from utils.utils_analyze import (plot_history, plot_history_statistics, plot_weights, plot_decoded, plot_decoded_exp,\n",
    "                                 plot_latent_representation, plot_predicted, plot_exp_statistics, get_threshold,\n",
    "                                 reject_outliers)\n",
    "from utils.utils_sld import plot_SLD\n",
    "\n",
    "# reproducibility\n",
    "seed = 12\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edba87",
   "metadata": {},
   "source": [
    "### Load and process synthetic data\n",
    "\n",
    "- Input the `sample_name` and the `set` number of the dataset to locate the appropriate directories and read the metadata.\n",
    "\n",
    "\n",
    "- Load and process (apply noise, smoothing, and normalization to) spectra and identify the parameter names `y_header` and corresponding indices `y_ids` to use for supervised learning.\n",
    "\n",
    "\n",
    "- Standardize `y_data` (parameter values), perform train/valid/test split stratified by class, and convert data to torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data properties\n",
    "sample_name = 'BiSe10_EuS5' #'CrO20_BiSbTe20'\n",
    "dataset = 0\n",
    "exclude_s = False\n",
    "exclude_inst = True\n",
    "add_mt = False\n",
    "delta = 0.5\n",
    "\n",
    "# get directories\n",
    "set_dir = 'results/' + sample_name + '/set_' + str(dataset)\n",
    "data_dir = set_dir + '/data'\n",
    "\n",
    "# parse metadata and experiment files\n",
    "layers, rho, M, N, q_min, q_max = parse_metadata(set_dir)\n",
    "q = 10*np.linspace(q_min, q_max, N)\n",
    "exp_names = get_exp_names(sample_name)\n",
    "\n",
    "print('set directory:', set_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process data\n",
    "x_data, x_orig, y_data, y_columns, y_header, y_ids, \\\n",
    "y_labels, y_units, y_units_ = process_data(data_dir, sample_name, q, seed, delta, exclude_s, exclude_inst, add_mt)\n",
    "\n",
    "# split data\n",
    "idx_train, idx_valid, idx_test = split_data(x_data, seed)\n",
    "\n",
    "# log scale and normalize\n",
    "x_train, x_moms = normalize_log(x_data[idx_train])\n",
    "x_valid, _ = normalize_log(x_data[idx_valid], x_moms)\n",
    "x_test, _ = normalize_log(x_data[idx_test], x_moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "x_train = torch.from_numpy(x_train).unsqueeze(1)\n",
    "x_valid = torch.from_numpy(x_valid).unsqueeze(1)\n",
    "x_test = torch.from_numpy(x_test).unsqueeze(1)\n",
    "y_train = torch.from_numpy(y_data[idx_train])\n",
    "y_valid = torch.from_numpy(y_data[idx_valid])\n",
    "y_test = torch.from_numpy(y_data[idx_test])\n",
    "\n",
    "height = x_train.size()[2]\n",
    "width = x_train.size()[3]\n",
    "num_features = len(y_ids)\n",
    "prox_features = [k for k, v in enumerate(y_header) if 'prox' in v]\n",
    "print('height:', height, 'width:', width, 'num_features:', num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "scaler = TorchStandardScaler()\n",
    "scaler.fit(y_train[:,y_ids])\n",
    "y_train[:,np.array(y_ids)] = scaler.transform(y_train[:,y_ids])\n",
    "y_valid[:,np.array(y_ids)] = scaler.transform(y_valid[:,y_ids])\n",
    "y_test[:,np.array(y_ids)] = scaler.transform(y_test[:,y_ids])\n",
    "\n",
    "scaler.mean = scaler.mean.to(device)\n",
    "scaler.std = scaler.std.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7037c3",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "- Input the `model name` (vae, cvae) for the model to train according to:\n",
    "    * vae: variational autoencoder (VAE)\n",
    "    * cvae: VAE conditioned on parameters in `y_header`\n",
    "\n",
    "\n",
    "- Input the `model_num` to resume training (or to continue analysis without further fitting), or -1 to start a new fit.\n",
    "\n",
    "\n",
    "- Input the number of models (`reps`) to train in succession, and set the `batch_size` and number of `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484c5f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model to train\n",
    "model_name = 'cvae'\n",
    "model_num = -1            # -1 to start a new fit, model number to resume or analyze\n",
    "\n",
    "# training parameters\n",
    "reps = 10\n",
    "batch_size = 512\n",
    "epochs = 100\n",
    "z_std_norm = False\n",
    "\n",
    "# model parameters\n",
    "kwargs = {\n",
    "    'latent_dim': 24,\n",
    "    'start_filters': 16,\n",
    "    'kernel_size': 7,\n",
    "    'pool_size': 4,\n",
    "    'num_conv': 2,\n",
    "    'num_dense': 4,\n",
    "    'slope': 0.2,\n",
    "    'drop': False,\n",
    "    'beta_1': 0.01,\n",
    "    'beta_2': 0.05\n",
    "}\n",
    "\n",
    "# print model architecture\n",
    "model, _, _ = init_model(model_name, height, width, num_features, kwargs, device, scaler=scaler)\n",
    "print(model)\n",
    "print('number of parameters:', sum([p.numel() for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model directories\n",
    "model_prefix = '/' + model_name + '_'\n",
    "resume_fit = model_num >= 0\n",
    "\n",
    "if resume_fit:\n",
    "    # load args from saved model directory\n",
    "    model_dir = set_dir + model_prefix + str(model_num)\n",
    "    args = torch.load(model_dir + '/model_0/model.torch', map_location='cpu')['args']\n",
    "    start_epoch = args['epochs']\n",
    "    epochs = start_epoch + epochs\n",
    "    args['epochs'] = epochs\n",
    "    \n",
    "    z_std_norm = args['z_std_norm']\n",
    "    kwargs = args['kwargs']\n",
    "    \n",
    "else:\n",
    "    # create new model directory\n",
    "    args = {}\n",
    "    dirs = next(os.walk(set_dir))[1]\n",
    "    for k in dirs:\n",
    "        if k.startswith('.'): dirs.remove(k)\n",
    "    dirs.remove('data')\n",
    "    if 'properties' in dirs: dirs.remove('properties')\n",
    "\n",
    "    if len(dirs):\n",
    "        idns = [int(d.split('_')[-1]) for d in dirs]\n",
    "        idn = max(idns) + 1\n",
    "        args['model_num'] = idn\n",
    "        model_dir = set_dir + model_prefix + str(idn)\n",
    "    else:\n",
    "        args['model_num'] = 0\n",
    "        model_dir = set_dir + model_prefix + '0'\n",
    "\n",
    "    os.makedirs(model_dir)\n",
    "    start_epoch = 0\n",
    "    \n",
    "    # save arguments\n",
    "    args['batch_size'] = batch_size\n",
    "    args['reps'] = reps\n",
    "    args['epochs'] = epochs\n",
    "    args['z_std_norm'] = z_std_norm\n",
    "    args['kwargs'] = kwargs.copy()\n",
    "    \n",
    "meta = json.dumps(args, sort_keys=True, indent=2)\n",
    "with open(model_dir + '/args.txt', 'w') as f:\n",
    "    f.write(meta)\n",
    "\n",
    "print('model directory:', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c208575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure data loaders\n",
    "data_train = TensorDataset(x_train, y_train)\n",
    "data_valid = TensorDataset(x_valid, y_valid)\n",
    "data_test = TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(data_valid, batch_size=batch_size)\n",
    "test_loader = DataLoader(data_test, batch_size=batch_size)\n",
    "\n",
    "d_sets = ['train', 'valid', 'test']\n",
    "data_loaders = dict(zip(d_sets, [DataLoader(data_train, batch_size=batch_size), valid_loader, test_loader]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3f024",
   "metadata": {},
   "source": [
    "### Train model\n",
    "\n",
    "- Train reps number of models in succession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1ec6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = 5\n",
    "for r in range(reps):\n",
    "    image_dir = model_dir + '/model_' + str(r)\n",
    "    if not os.path.exists(image_dir): os.makedirs(image_dir)\n",
    "\n",
    "    model, opt, metric_keys = init_model(model_name, height, width, num_features, kwargs, device, scaler=scaler)\n",
    "\n",
    "    print('====== repetition:', r)\n",
    "\n",
    "    if resume_fit:\n",
    "        model.load_state_dict(torch.load(image_dir + '/model.torch', map_location=device)['state'])\n",
    "        opt.load_state_dict(torch.load(image_dir + '/model.torch', map_location=device)['optimizer'])\n",
    "        dynamics = torch.load(image_dir + '/model.torch', map_location='cpu')['dynamics']\n",
    "    else: dynamics = []\n",
    "\n",
    "    for epoch in range(start_epoch + 1, epochs + 1):\n",
    "        train(epoch, model, opt, metric_keys, train_loader, device, model_name, y_ids, z_std_norm)\n",
    "        \n",
    "        if (epoch % checkpoint) == 0:\n",
    "            print(\"({}) Training:\".format(epoch))\n",
    "            train_metrics = evaluate(epoch, model, metric_keys, train_loader, device, model_name, y_ids, z_std_norm)\n",
    "            print(\"({}) Validation:\".format(epoch))\n",
    "            valid_metrics = evaluate(epoch, model, metric_keys, valid_loader, device, model_name, y_ids, z_std_norm)\n",
    "            dynamics.append({\n",
    "                'epoch': epoch,\n",
    "                'train': train_metrics,\n",
    "                'valid': valid_metrics\n",
    "            })\n",
    "\n",
    "            results = {\n",
    "                'args': args,\n",
    "                'dynamics': dynamics,\n",
    "                'state': model.state_dict(),\n",
    "                'optimizer': opt.state_dict()\n",
    "            }\n",
    "\n",
    "            # save data and metadata to .torch dictionary\n",
    "            with open(image_dir + '/model.torch', 'wb') as f:\n",
    "                torch.save(results, f)\n",
    "    \n",
    "    # save final results\n",
    "    results = {\n",
    "        'args': args,\n",
    "        'dynamics': dynamics,\n",
    "        'state': model.state_dict(),\n",
    "        'optimizer': opt.state_dict()\n",
    "    }\n",
    "\n",
    "    # save data and metadata to .torch dictionary\n",
    "    with open(image_dir + '/model.torch', 'wb') as f:\n",
    "        torch.save(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b693c",
   "metadata": {},
   "source": [
    "### Evaluate a representative model\n",
    "\n",
    "- Input the repetition `r` to load the corresponding trained model.\n",
    "\n",
    "\n",
    "- Load and process the experimental data.\n",
    "\n",
    "\n",
    "- Plot the training history.\n",
    "\n",
    "\n",
    "- Make predictions on all synthetic and experimental data.\n",
    "\n",
    "\n",
    "- Plot example reconstructions of synthetic data in each error quartile, and reconstructions of experimental data.\n",
    "\n",
    "\n",
    "- Visualize the latent space along 2 dimensions.\n",
    "\n",
    "\n",
    "- Plot performance of regressors, if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repetition to evaluate\n",
    "r = 0\n",
    "image_dir = model_dir + '/model_' + str(r)\n",
    "model, _, _ = init_model(model_name, height, width, num_features, kwargs, device, scaler=scaler)\n",
    "model.load_state_dict(torch.load(image_dir + '/model.torch', map_location=device)['state'])\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7d919",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot training history\n",
    "dynamics = torch.load(image_dir + '/model.torch', map_location='cpu')['dynamics']\n",
    "plot_history(image_dir, dynamics, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2D kernels\n",
    "plot_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on synthetic data\n",
    "df = get_predictions(model, data_loaders, ['test'], device, height, width, num_features, kwargs,\n",
    "                     model_name, y_ids, scaler)\n",
    "if model_name == 'cvae': df = convert_units(df, M, y_header, y_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6adac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on experimental data\n",
    "x_exp, dx_exp = process_exp(sample_name, exp_names, q, x_moms)\n",
    "x_exp = torch.from_numpy(x_exp).unsqueeze(1)\n",
    "df_exp = get_predictions_exp(model, x_exp, dx_exp, exp_names, device, height, width, num_features, kwargs,\n",
    "                             model_name, y_ids, scaler)\n",
    "if model_name == 'cvae':\n",
    "    df_exp = convert_units(df_exp, M, y_header)\n",
    "    \n",
    "    # plot predictions\n",
    "    fig, ax = plt.subplots(2, len(y_header)//2 + len(y_header)%2, figsize=(15,5.5))\n",
    "    ax = ax.ravel()\n",
    "    if len(exp_names) > 1: norm = plt.Normalize(vmin=0, vmax=len(exp_names)-1)\n",
    "    else: norm = plt.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    y_min = np.stack(df['y_true_'].values).min(axis=0)[y_ids]\n",
    "    y_max = np.stack(df['y_true_'].values).max(axis=0)[y_ids]\n",
    "\n",
    "    for i in range(len(exp_names)):\n",
    "        dict_exp = dict(zip(y_header, df_exp.iloc[i]['y_pred_']))\n",
    "        for j, (k, v) in enumerate(dict_exp.items()):\n",
    "            ax[j].scatter(0, v, s=48, color=cmap_temp(norm(i)), ec='black')\n",
    "        if i == 0:\n",
    "            for j, (l, k) in enumerate(zip(y_labels, y_units_)):\n",
    "                ax[j].set_xticks([])\n",
    "                if len(k) > 10: y_title = l + '\\n' + k\n",
    "                else: y_title = l + ' ' + k\n",
    "                ax[j].set_title(y_title, ha='center', loc='right', fontsize=fontsize)\n",
    "                ax[j].set_ylim([y_min[j] - 0.05*y_max[j], y_max[j]])\n",
    "                ax[j].tick_params(axis='y', labelsize=fontsize-1)\n",
    "                ax[j].yaxis.tick_right()\n",
    "                ax[j].spines['left'].set_visible(False)\n",
    "                ax[j].spines['top'].set_visible(False)\n",
    "                ax[j].spines['bottom'].set_visible(False)\n",
    "    for i in range(j+1,len(y_header)+1):\n",
    "        try: ax[i].remove()\n",
    "        except: break\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.5, wspace=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7409293e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale = 0\n",
    "ncol = 2\n",
    "show_prox = True\n",
    "for i in range(len(df_exp)):\n",
    "    y_pred = df_exp.iloc[i]['y_pred'].copy()\n",
    "    tag = df_exp.iloc[i]['set']\n",
    "    if i == 0: y_lims = plot_SLD(image_dir, sample_name, q, y_pred, y_header, tag, scale,\n",
    "                                 show_prox=show_prox, ncol=ncol)\n",
    "    else: plot_SLD(image_dir, sample_name, q, y_pred, y_header, tag, scale, y_lims, show_prox, ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04309c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot example reconstructions\n",
    "plot_decoded(image_dir, np.stack(df.loc[df['set']=='test', 'x_pred'].values),\n",
    "             np.stack(df.loc[df['set']=='test', 'x_true'].values),\n",
    "             np.stack(df.loc[df['set']=='test', 'x_mse'].values), 'test',\n",
    "             df_exp['x_mse'].values, exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d7256",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot reconstructed experiment\n",
    "plot_decoded_exp(image_dir, np.stack(df_exp['x_pred'].values), sample_name, exp_names, q, x_moms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize separability of latent space\n",
    "plot_latent_representation(image_dir, np.stack(df.loc[df['set']=='test', 'z'].values),\n",
    "                           np.stack(df.loc[df['set']=='test', 'y_true_'].values),\n",
    "                           y_ids, y_labels, y_units_, 'encoded_test', np.stack(df_exp['z'].values), exp_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9134025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of regressor predictions\n",
    "plot_predicted(image_dir, np.stack(df.loc[df['set']=='test', 'y_pred_'].values),\n",
    "               np.stack(df.loc[df['set']=='test', 'y_true_'].values), y_ids, y_labels, y_units_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2e171",
   "metadata": {},
   "source": [
    "### Evaluate all models\n",
    "\n",
    "- Compile the training history and experiment predictions on all model repetitions.\n",
    "\n",
    "\n",
    "- Plot the training history statistics.\n",
    "\n",
    "\n",
    "- Plot regression statistics on experimental data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b15375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on all models to compute statistics\n",
    "if model_name == 'cvae':\n",
    "    x_exp, dx_exp = process_exp(sample_name, exp_names, q, x_moms)\n",
    "    x_exp = torch.from_numpy(x_exp).unsqueeze(1)\n",
    "\n",
    "    dynamics = []\n",
    "    for r in range(reps):\n",
    "        # load model\n",
    "        image_dir = model_dir + '/model_' + str(r)\n",
    "        model, _, _ = init_model(model_name, height, width, num_features, kwargs, device, scaler=scaler)\n",
    "        model.load_state_dict(torch.load(image_dir + '/model.torch', map_location=device)['state'])\n",
    "        model.eval()\n",
    "\n",
    "        # get training history\n",
    "        dynamics += [torch.load(image_dir + '/model.torch', map_location='cpu')['dynamics']]\n",
    "\n",
    "        # predict on synthetic data\n",
    "        df = get_predictions(model, data_loaders, ['valid'], device, height, width, num_features, kwargs,\n",
    "                             model_name, y_ids, scaler)\n",
    "        df = convert_units(df, M, y_header, y_columns)\n",
    "        \n",
    "        y_true = np.stack(df.loc[df['set']=='valid', 'y_true_'].values)\n",
    "        y_pred = np.stack(df.loc[df['set']=='valid', 'y_pred_'].values)\n",
    "    \n",
    "        k = y_header.index('d_prox')\n",
    "        d_prox_th = get_threshold(image_dir, y_true[:,y_ids[k]], y_pred[:,k], y_labels[k], y_units_[k], 'd_prox')\n",
    "        \n",
    "        k = y_header.index('magn_prox')\n",
    "        m_prox_th = get_threshold(image_dir, y_true[:,y_ids[k]], y_pred[:,k], y_labels[k], y_units_[k], 'magn_prox')\n",
    "        \n",
    "        if 'd_iAFM' in y_header:\n",
    "            k = y_header.index('d_iAFM')\n",
    "            d_iAFM_th = get_threshold(image_dir, y_true[:,y_ids[k]], y_pred[:,k], y_labels[k], y_units_[k], 'd_iAFM')\n",
    "            \n",
    "            k = y_header.index('magn_iAFM')\n",
    "            m_iAFM_th = get_threshold(image_dir, y_true[:,y_ids[k]], y_pred[:,k], y_labels[k], y_units_[k], 'magn_iAFM')\n",
    "\n",
    "        if r > 0:\n",
    "            df_ = get_predictions_exp(model, x_exp, dx_exp, exp_names, device, height, width, num_features, kwargs,\n",
    "                                      model_name, y_ids, scaler)\n",
    "            df_ = convert_units(df_, M, y_header)\n",
    "            df_['model'] = r; df_['d_prox_th'] = d_prox_th; df_['m_prox_th'] = m_prox_th\n",
    "            try: df_['d_iAFM_th'] = d_iAFM_th\n",
    "            except: pass\n",
    "            else: df_['m_iAFM_th'] = m_iAFM_th\n",
    "            df_exp = df_exp.append(df_, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            # predict on experimental data\n",
    "            df_exp = get_predictions_exp(model, x_exp, dx_exp, exp_names, device, height, width, num_features, kwargs,\n",
    "                                         model_name, y_ids, scaler)\n",
    "            df_exp = convert_units(df_exp, M, y_header)\n",
    "            df_exp['model'] = r; df_exp['d_prox_th'] = d_prox_th; df_exp['m_prox_th'] = m_prox_th\n",
    "            try: df_exp['d_iAFM_th'] = d_iAFM_th\n",
    "            except: pass\n",
    "            else: df_exp['m_iAFM_th'] = m_iAFM_th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01b81f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot training history statistics\n",
    "plot_history_statistics(model_dir, dynamics, logscale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5323c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_min = np.stack(df['y_true_'].values).min(axis=0)[y_ids]\n",
    "y_max = np.stack(df['y_true_'].values).max(axis=0)[y_ids]\n",
    "\n",
    "for i, y_name in enumerate(y_header):\n",
    "    if y_name == 'd_prox': y_th = np.mean(reject_outliers(df_exp['d_prox_th'].values))\n",
    "    elif (y_name == 'magn_prox') or (y_name == 'magn_FM'): y_th = np.mean(reject_outliers(df_exp['m_prox_th'].values))\n",
    "    elif y_name == 'd_iAFM': y_th = np.mean(reject_outliers(df_exp['d_iAFM_th'].values))\n",
    "    elif y_name == 'magn_iAFM': y_th = np.mean(reject_outliers(df_exp['m_iAFM_th'].values))\n",
    "    else: y_th = None\n",
    "    ylims = [y_min[i], y_max[i]]\n",
    "    plot_exp_statistics(model_dir, df_exp, reps, y_name, y_header, y_labels, y_units_, y_th=y_th, y_lims=ylims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd430d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
